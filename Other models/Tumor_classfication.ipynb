{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tumor_classfication.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"nL9g-tI-oidc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604953770714,"user_tz":300,"elapsed":694780,"user":{"displayName":"Arjun Sarathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw5nf2AeKKZ-B0IjUYUMPh6DZR07VjPc7-UwZ1=s64","userId":"13702711285636645201"}},"outputId":"e3de9972-a763-40fe-81cc-72b087f447f0"},"source":["import os\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","os.environ['KAGGLE_USERNAME'] = \"haodong1997\"\n","os.environ['KAGGLE_KEY'] = \"f31c6aa3982c597ae202acc8a1427559\"\n","\n","!pip install --upgrade --force-reinstall --no-deps kaggle\n","\n","# Download the dataset.zip into the current run time. (Check /content, it should be there)\n","!kaggle competitions download -c histopathologic-cancer-detection\n","!cp /content/histopathologic-cancer-detection.zip /content/\n","!unzip -qq /content/histopathologic-cancer-detection.zip -d /content/\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","cp: cannot stat '/content/histopathologic-cancer-detection.zip': No such file or directory\n","unzip:  cannot find or open /content/histopathologic-cancer-detection.zip, /content/histopathologic-cancer-detection.zip.zip or /content/histopathologic-cancer-detection.zip.ZIP.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z81IRLEbG877"},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","#from sklearn.utils import shuffle\n","import matplotlib.patches as patches\n","import cv2\n","import shutil\n","\n","#Needed for correct pathing with different operating systems\n","from pathlib import Path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UyQwPAZr3co0"},"source":["#Setting up paths to images & labels of data (may need to change the paths)\n","data_path = Path('/content/drive/MyDrive/Comp_Medicine/project_data/train_labels.csv')\n","data = pd.read_csv(data_path)\n","train_path = Path('/content/drive/MyDrive/Comp_Medicine/project_data/train/')\n","test_path = Path('/content/drive/MyDrive/Comp_Medicine/project_data/test/')\n","\n","#Make new folders. Remake them if they already exist (wipe data)\n","\n","temp_dir = '/content/drive/MyDrive/Comp_Medicine/project_data_split/train/positive/'\n","if os.path.exists(temp_dir):\n","  shutil.rmtree(temp_dir)\n","else:\n","  os.makedirs(temp_dir)\n","\n","temp_dir = '/content/drive/MyDrive/Comp_Medicine/project_data_split/train/negative/'\n","if os.path.exists(temp_dir):\n","  shutil.rmtree(temp_dir)\n","else:\n","  os.makedirs(temp_dir)\n","\n","temp_dir = '/content/drive/MyDrive/Comp_Medicine/project_data_split//val/positive/'\n","if os.path.exists(temp_dir):\n","  shutil.rmtree(temp_dir)\n","else:\n","  os.makedirs(temp_dir)\n","\n","temp_dir = '/content/drive/MyDrive/Comp_Medicine/project_data_split/val/negative/'\n","if os.path.exists(temp_dir):\n","  shutil.rmtree(temp_dir)\n","else:\n","  os.makedirs(temp_dir)\n","\n","data_train_pos_path = Path('/content/drive/MyDrive/Comp_Medicine/project_data_split/train/positive/')\n","data_train_neg_path =Path('/content/drive/MyDrive/Comp_Medicine/project_data_split/train/negative/')\n","data_val_pos_path = Path('/content/drive/MyDrive/Comp_Medicine/project_data_split//val/positive/')\n","data_val_neg_path = Path('/content/drive/MyDrive/Comp_Medicine/project_data_split/val/negative/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6Dbo6Nx3jSH"},"source":["#Showing the visualizations of an image.\n","\n","fig, ax = plt.subplots(1,3, figsize=(20,8))\n","img=plt.imread('/content/drive/MyDrive/Comp_Medicine/project_data/train/0000da768d06b879e5754c43e2298ce48726f722.tif')\n","\n","#Regular image using matplotlib.pyplot\n","ax[0].set_title('Normal')\n","ax[0].imshow(img)\n","\n","#CV2 processes the image as a BGR image, so the colors are 'incorrect'.\n","img2 = cv2.imread('/content/drive/MyDrive/Comp_Medicine/project_data/train/0000da768d06b879e5754c43e2298ce48726f722.tif')\n","ax[1].set_title('CV2 Default (BGR)')\n","ax[1].imshow(img2)\n","\n","#To fix, we can just split and reorder the image to standard RGB.\n","b,g,r = cv2.split(img2)\n","rgb_img2 = cv2.merge([r,g,b])\n","ax[2].set_title('CV2 RGB')\n","ax[2].imshow(rgb_img2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OLVzSgXP3nR6"},"source":["#In this challenge, we are tasked to examine the 32x32 px center of the full 96x96 px image.\n","fig, ax = plt.subplots(figsize = (20,8))\n","ax.imshow(img)\n","focus = patches.Rectangle((32,32),32,32, linewidth = 3, edgecolor = 'g',facecolor='none')\n","ax.add_patch(focus)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"biDrfZ4_GmA-"},"source":["# Preprocessing\n","\n","For now, we will just preprocess the training data by cropping the image for the 32x32 center. Of course, this means we are cutting out potential information, but we can go back to this later. We can also prevent overfitting the model by performing random minor augmentations/perturbations to the image for the model to train on.\n","\n","**Make sure that you have the folders created**"]},{"cell_type":"markdown","metadata":{"id":"QefqjdCbGrpd"},"source":["## Image Cropping"]},{"cell_type":"code","metadata":{"id":"2z8Smv-EGxE2"},"source":["#Image Cropping Method\n","\n","fig, ax = plt.subplots(1,2,figsize = (20,8))\n","img=plt.imread('/content/drive/MyDrive/Comp_Medicine/project_data/train/0000da768d06b879e5754c43e2298ce48726f722.tif')\n","ax[0].imshow(img)\n","focus = patches.Rectangle((32,32),32,32, linewidth = 3, edgecolor = 'g',facecolor='none')\n","ax[0].add_patch(focus)\n","\n","img_cropped = img[31:64,31:64].copy()\n","ax[1].imshow(img_cropped)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oNpkQuUOPvre"},"source":["num_data = len(data)\n","#Shuffle Data\n","data2 = data.sample(frac = 1).reset_index(drop=True)\n","#Split data into training and validation sets\n","num_val_data = num_data//10\n","data_val = data2.iloc[:num_val_data,:]\n","data_train = data2.iloc[num_val_data:,:].reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WfIbEbkQGQU"},"source":["#Preprocess and save images in validation set\n","for ind in data_val.index:\n","    img_id = str(data_val['id'][ind]) + '.tif'\n","    img_path = train_path / img_id\n","    \n","    #print(img_path)\n","    if (data_val['label'][ind] == 0):\n","        new_img_path = data_val_neg_path / img_id\n","    else:\n","        new_img_path = data_val_pos_path / img_id\n","    \n","    img=cv2.imread(str(img_path))\n","    img_cropped = img[31:64,31:64].copy()\n","    cv2.imwrite(str(new_img_path), img_cropped)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UyUsMPblE5k8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekpXZoQrQJKo"},"source":["#Preprocess and save images in training set\n","for ind in data_train.index:\n","    img_id = str(data_train['id'][ind]) + '.tif'\n","    img_path = train_path / img_id\n","    \n","    #print(img_path)\n","    if (data_train['label'][ind] == 0):\n","        new_img_path = data_train_neg_path / img_id\n","    else:\n","        new_img_path = data_train_pos_path / img_id\n","    \n","    img=cv2.imread(str(img_path))\n","    img_cropped = img[31:64,31:64].copy()\n","    cv2.imwrite(str(new_img_path), img_cropped)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGBCRa4xQMch"},"source":["#To Clear all data from preprocessed folders, run this block.\n","#shutil.rmtree(data_val_neg_path)\n","#os.makedirs(data_val_neg_path)\n","\n","#shutil.rmtree(data_val_pos_path)\n","#os.makedirs(data_val_pos_path)\n","\n","#shutil.rmtree(data_train_neg_path)\n","#os.makedirs(data_train_neg_path)\n","\n","#shutil.rmtree(data_train_pos_path)\n","#os.makedirs(data_train_pos_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wn9Z35tyGznw"},"source":["## Elimination\n","Get rid of very dark and very bright images like the ones seen below. (Or perhaps mark them as negative...)"]},{"cell_type":"code","metadata":{"id":"JZzXq9L8G5Ey"},"source":["# For example, these images are either too dark or too bright to be useful.\n","\n","fig, ax = plt.subplots(1,2, figsize=(20,8))\n","preimg = cv2.imread('/content/drive/MyDrive/Comp_Medicine/project_data/train/9369c7278ec8bcc6c880d99194de09fc2bd4efbe.tif')\n","b,g,r = cv2.split(preimg)\n","img = cv2.merge([r,g,b])\n","ax[0].set_title('Dark')\n","ax[0].imshow(img)\n","\n","preimg2 = cv2.imread('/content/drive/MyDrive/Comp_Medicine/project_data/train/f6f1d771d14f7129a6c3ac2c220d90992c30c10b.tif')\n","b,g,r = cv2.split(preimg2)\n","img2 = cv2.merge([r,g,b])\n","ax[1].set_title('Bright')\n","ax[1].imshow(img2)"],"execution_count":null,"outputs":[]}]}