{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tumor_classfication_model_HVflip_Res54.ipynb","provenance":[],"collapsed_sections":["RdMFa1C62RsD","2nJTwTmLSb1V"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7KYd7FRDccZt","executionInfo":{"status":"ok","timestamp":1610726583709,"user_tz":300,"elapsed":498,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}},"outputId":"99f0d0bc-2384-4cd1-ad82-2a45f4eadcf0"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(\"V100 is three times faster than P100\")\n","  print(gpu_info)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["V100 is three times faster than P100\n","Fri Jan 15 16:03:03 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    39W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nL9g-tI-oidc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610726583848,"user_tz":300,"elapsed":622,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}},"outputId":"92563145-524f-4622-c24b-9c2826141b10"},"source":["import os\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","os.environ['KAGGLE_USERNAME'] = \"haodong1997\"\n","os.environ['KAGGLE_KEY'] = \"f31c6aa3982c597ae202acc8a1427559\"\n","\n","# !pip install --upgrade --force-reinstall --no-deps kaggle\n","\n","# # # Download the dataset.zip into the current run time. (Check /content, it should be there)\n","# !kaggle competitions download -c histopathologic-cancer-detection\n","\n","# !cp /content/histopathologic-cancer-detection.zip /content/\n","# !rm -r /content/test\n","# !mkdir -p /content/test/folder\n","\n","# !unzip -qq /content/histopathologic-cancer-detection.zip \"test/*\" -d /content/test/folder\n","\n","# # Download the filtered data\n","# ! curl -L --output data.zip https://www.dropbox.com/s/qjkr9fxxe3bnw68/data_noCrop.zip?dl=1\n","# !unzip -qq /content/data.zip"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qpz6hdiQUqvv","executionInfo":{"status":"ok","timestamp":1610726583849,"user_tz":300,"elapsed":613,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}}},"source":["# https://www.kaggle.com/artgor/simple-eda-and-model-in-pytorch\n","# https://www.kaggle.com/qitvision/a-complete-ml-pipeline-fast-ai\n","# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"z81IRLEbG877","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610726584963,"user_tz":300,"elapsed":1714,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}},"outputId":"a4c8333e-64e3-49cd-bd1c-e4a101c6f6ca"},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","#from sklearn.utils import shuffle\n","import matplotlib.patches as patches\n","import cv2\n","from pathlib import Path #Needed for correct pathing with different operating systems\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve, auc\n","from __future__ import print_function\n","from __future__ import division\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","from copy import deepcopy\n","from fastai.vision import *\n","from torchvision.models import *\n","\n","print(\"PyTorch Version: \", torch.__version__)\n","print(\"Torchvision Version: \", torchvision.__version__)   # Change it 0.4.0 if problem"],"execution_count":4,"outputs":[{"output_type":"stream","text":["PyTorch Version:  1.7.0+cu101\n","Torchvision Version:  0.8.1+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nVQBq9Qerber","executionInfo":{"status":"ok","timestamp":1610726584966,"user_tz":300,"elapsed":1706,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}}},"source":["# !pip install torch==1.2.0 torchvision==0.4.0"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S7lYDt_ug2R8"},"source":["# Parameters"]},{"cell_type":"code","metadata":{"id":"pKSxrXbHm1LB","executionInfo":{"status":"ok","timestamp":1610726584966,"user_tz":300,"elapsed":1698,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}}},"source":["# Top level data directory. Here we assume the format of the directory conforms\n","#   to the ImageFolder structure\n","# [resnet, alexnet, vgg, squeezenet, densenet, inception]\n","\n","# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n","model_name = \"resnet\"\n","\n","# Number of classes in the dataset\n","num_classes = 2\n","\n","# Batch size for training (change depending on how much memory you have)\n","batch_size = 16\n","\n","# Number of epochs to train for\n","num_epochs = 20\n","\n","# Flag for feature extracting. When False, we finetune the whole model,\n","# when True we only update the reshaped layer params\n","feature_extract = False\n","# feature_extract = True"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"QozYJnPp4NRw","executionInfo":{"status":"ok","timestamp":1610726584967,"user_tz":300,"elapsed":1692,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}}},"source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RdMFa1C62RsD"},"source":["#Initialize Model"]},{"cell_type":"code","metadata":{"id":"dEsrREya2TVY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610726585335,"user_tz":300,"elapsed":2053,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}},"outputId":"8864528f-ade2-4499-e479-c04d3025c53c"},"source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18\n","        \"\"\"\n","        model_ft = models.resnet18(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"alexnet\":\n","        \"\"\" Alexnet\n","        \"\"\"\n","        model_ft = models.alexnet(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"squeezenet\":\n","        \"\"\" Squeezenet\n","        \"\"\"\n","        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n","        model_ft.num_classes = num_classes\n","        input_size = 224\n","\n","    elif model_name == \"densenet\":\n","        \"\"\" Densenet\n","        \"\"\"\n","        model_ft = models.densenet121(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"inception\":\n","        \"\"\" Inception v3\n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        num_ftrs = model_ft.AuxLogits.fc.in_features\n","        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","        input_size = 299\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","\n","    return model_ft, input_size\n","\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(\"resnet\", num_classes, feature_extract, use_pretrained=True)\n","\n","# Print the model we just instantiated\n","print(model_ft)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=2, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kHyMCmO3YlAg"},"source":["# Resnet"]},{"cell_type":"code","metadata":{"id":"jdVSe0GoYjXE","executionInfo":{"status":"ok","timestamp":1610726585905,"user_tz":300,"elapsed":2612,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}}},"source":["class BasicBlock(nn.Module):\n","    expansion = 1\n","    def __init__(self, inChannel, outChannel, stride = 1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(inChannel, outChannel, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(outChannel)\n","        self.conv2 = nn.Conv2d(outChannel, outChannel, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(outChannel)\n","\n","        self.shortcut = nn.Sequential()\n","        # Fix dimension to match different block\n","        if (stride != 1) or (inChannel != self.expansion * outChannel):\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(inChannel, self.expansion * outChannel, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(outChannel)\n","                )\n","            \n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","\n","def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","class Bottleneck(nn.Module):\n","    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n","    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n","    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n","    # This variant is also known as ResNet V1.5 and improves accuracy according to\n","    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n","\n","    expansion: int = 4\n","\n","    def __init__(\n","        self,\n","        inplanes: int,\n","        planes: int,\n","        stride: int = 1,\n","        downsample: Optional[nn.Module] = None,\n","        groups: int = 1,\n","        base_width: int = 64,\n","        dilation: int = 1,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None\n","    ) -> None:\n","        super(Bottleneck, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.)) * groups\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Network(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes):\n","        super(Network, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(25088, num_classes)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out) \n","        out = self.layer4(out)\n","        # out = self.avgpool(out)\n","        out = F.avg_pool2d(out, 4)\n","        embedding_out = out.view(out.size(0), -1)\n","        # print(\"embedding_out in forward\",embedding_out.shape)\n","        classification_out = self.linear(embedding_out)\n","\n","        # print(\"out in forward\",out.shape)\n","\n","        return classification_out\n","\n","input_size = 224\n","model_ft = Network(BasicBlock, [3, 4, 6, 3], num_classes)\n","\n","# print(model_ft)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FOKm3ELMnnuB"},"source":["# Datast Loading"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPnnB6czg01K","executionInfo":{"status":"ok","timestamp":1610726586940,"user_tz":300,"elapsed":3641,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}},"outputId":"2912e4bd-e5b9-4e5f-8acb-5b941adf0bd1"},"source":["# Data augmentation and normalization for training\n","# Just normalization for validation\n","\n","# For imaging\n","rgb_channels_mean = [0.702447, 0.546243, 0.696453]\n","rgb_channels_SD = [0.238893, 0.282094, 0.216251]\n","\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize(input_size),\n","        # transforms.CenterCrop(input_size),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(rgb_channels_mean, rgb_channels_SD)\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(input_size),\n","        # transforms.CenterCrop(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(rgb_channels_mean, rgb_channels_SD)\n","    ]),\n","    'test': transforms.Compose([\n","        # transforms.CenterCrop(32),  #cut the center part of data\n","        transforms.Resize(input_size),\n","        # transforms.CenterCrop(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(rgb_channels_mean, rgb_channels_SD)\n","    ]),\n","}\n","\n","# data --> trans --> model\n","print(\"Initializing Datasets and Dataloaders...\")\n","\n","# Create training and validation datasets dictionary\n","data_dir = \"/content/data\"\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n","\n","# Create training and validation dataloaders dictionary\n","dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=8) for x in ['train', 'val']}\n","\n","# Test dataset and loader\n","test_dir = \"/content/test/folder\"\n","test_dataset = torchvision.datasets.ImageFolder(root=test_dir, transform=data_transforms[\"test\"])\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n","\n","\n","# Detect if we have a GPU available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(\"Initialization Done!\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Initializing Datasets and Dataloaders...\n","Initialization Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-vd4ZRK9nkBq"},"source":["# Training & evaluation function "]},{"cell_type":"code","metadata":{"id":"v4lLm9Ttnjjv","executionInfo":{"status":"ok","timestamp":1610726586941,"user_tz":300,"elapsed":3635,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}}},"source":["def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","    val_loss_history = []\n","\n","    best_model_wts = deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        start_time = time.time()\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print(\"current learning:\", get_lr(optimizer))\n","        print('-' * 20)     \n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            true_labels = np.array([])\n","            probs = np.array([])\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss # Special case for inception\n","                    if is_inception and phase == 'train':\n","                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4*loss2\n","                    else:\n","                        # print(\"inputs.shape\", inputs.shape)\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","                    \n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                \n","                \n","                true_labels = np.append(true_labels, labels.cpu())\n","                probs = np.append(probs, outputs[:,-1].cpu().detach().numpy())\n","            \n","            auc_scores = roc_auc_score(true_labels , probs) \n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            \n","            print('{} Loss: {:.4f} Accuracy: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","            print(\"AUC score: {}\".format(auc_scores), \"\\n\")\n","        \n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                print(\"New best acc! Save model\")\n","                best_acc = epoch_acc\n","                best_model_wts = deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","                val_loss_history.append(epoch_loss)\n","\n","        print(\"Used time: {}\".format(str((time.time() - start_time)/60) + \" min\"))\n","\n","        if auc_scores > 0.99:\n","            torch.save({'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scheduler_state_dict' : scheduler.state_dict(),\n","                    }, \"/content/drive/MyDrive/Comp_Medicine/Model/Resnet18_HVflip_better\"+str(epoch))\n","            print(\"auc_scores > 0.96, model saved\")\n","            break\n","\n","        torch.cuda.empty_cache()\n","        del inputs\n","        del labels\n","        del loss\n","        scheduler.step()\n","        print()\n","\n","    \n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m'.format(time_elapsed / 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history, val_loss_history"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kOVLFLTjkLHA"},"source":["# Optimizer, scheduler, criterion"]},{"cell_type":"code","metadata":{"id":"GbsVmvNvkNHs","executionInfo":{"status":"ok","timestamp":1610726590722,"user_tz":300,"elapsed":7409,"user":{"displayName":"Haodong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2V3yTg6Gtjp8o1G2Y_zZaHgJ-SuvkND7-2NzDBQ=s64","userId":"02237120692416897823"}}},"source":["# Send the model to GPU\n","model_ft = model_ft.to(device)\n","\n","#  Gather the parameters to be optimized/updated in this run. If we are\n","#  finetuning we will be updating all parameters. However, if we are\n","#  doing feature extract method, we will only update the parameters\n","#  that we have just initialized, i.e. the parameters with requires_grad\n","#  is True.\n","params_to_update = model_ft.parameters()\n","# print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            # print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            # print(\"\\t\",name)\n","            continue\n","\n","# Observe that all parameters are being optimized\n","learningRate = 0.01\n","weightDecay = 5e-5\n","momentum=0.9\n","optimizer = torch.optim.SGD(model_ft.parameters(), lr=0.001, weight_decay=weightDecay, momentum=momentum)\n","\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.85)\n","\n","criterion = nn.CrossEntropyLoss()"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QarLMmbpoqCW"},"source":["#Model training and evaluate\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pF7tM0yio07U","outputId":"f4892cfd-eaef-4bb7-940b-7e70c62ba493"},"source":["model_out, val_acc_history, val_loss_history = train_model(model_ft, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0/19\n","current learning: 0.001\n","--------------------\n","train Loss: 0.4496 Accuracy: 0.8200\n","AUC score: 0.8865612790543951 \n","\n","val Loss: 0.6432 Accuracy: 0.8487\n","AUC score: 0.9039016364353961 \n","\n","New best acc! Save model\n","Used time: 107.91373012463252 min\n","\n","Epoch 1/19\n","current learning: 0.001\n","--------------------\n","train Loss: 0.3253 Accuracy: 0.8636\n","AUC score: 0.9324608334330996 \n","\n","val Loss: 0.4882 Accuracy: 0.8926\n","AUC score: 0.9459874836282355 \n","\n","New best acc! Save model\n","Used time: 107.83262378374735 min\n","\n","Epoch 2/19\n","current learning: 0.001\n","--------------------\n","train Loss: 0.2707 Accuracy: 0.8894\n","AUC score: 0.9531522246355113 \n","\n","val Loss: 0.8647 Accuracy: 0.8407\n","AUC score: 0.9507983705359838 \n","\n","Used time: 107.78300100564957 min\n","\n","Epoch 3/19\n","current learning: 0.00085\n","--------------------\n","train Loss: 0.2274 Accuracy: 0.9105\n","AUC score: 0.9664538161987126 \n","\n","val Loss: 0.2478 Accuracy: 0.9238\n","AUC score: 0.9716795063110201 \n","\n","New best acc! Save model\n","Used time: 107.71896422306696 min\n","\n","Epoch 4/19\n","current learning: 0.00085\n","--------------------\n","train Loss: 0.2074 Accuracy: 0.9204\n","AUC score: 0.9717361791556708 \n","\n","val Loss: 0.2312 Accuracy: 0.9336\n","AUC score: 0.977474423063411 \n","\n","New best acc! Save model\n","Used time: 107.7316166917483 min\n","\n","Epoch 5/19\n","current learning: 0.00085\n","--------------------\n","train Loss: 0.1819 Accuracy: 0.9316\n","AUC score: 0.977862602752626 \n","\n","val Loss: 1.9161 Accuracy: 0.9393\n","AUC score: 0.9774760784230848 \n","\n","New best acc! Save model\n","Used time: 107.73552041053772 min\n","\n","Epoch 6/19\n","current learning: 0.0007224999999999999\n","--------------------\n","train Loss: 0.1618 Accuracy: 0.9407\n","AUC score: 0.982169528193281 \n","\n","val Loss: 2.4992 Accuracy: 0.9368\n","AUC score: 0.970142217524622 \n","\n","Used time: 107.71465771595636 min\n","\n","Epoch 7/19\n","current learning: 0.0007224999999999999\n","--------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PwelmQdiiGdR"},"source":["# plt.plot(val_acc_history, label = \"val_acc\")\n","# plt.plot(val_loss_history, label = \"val_loss\")\n","# plt.xlabel(\"training epoch\")\n","# plt.legend()\n","\n","\n","# dir = \"/content/drive/MyDrive/Comp_Medicine/output_fig/\"\n","# plt.savefig(dir + \"resnet_HVflip.jpg\")\n","\n","# plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2nJTwTmLSb1V"},"source":["#Testing funtion"]},{"cell_type":"code","metadata":{"id":"epxDlwvdSeS7"},"source":["def test_classify(model, test_loader):\n","    model.eval()\n","    probs_list = []\n","    with torch.no_grad():\n","        for batch_num, (feats, labels) in enumerate(test_loader):\n","            feats, labels = feats.to(device), labels.to(device)\n","            outputs = model(feats).cpu()\n","\n","            # Extract prob\n","            probs = outputs[:,1].detach().cpu().numpy()\n","            probs_list.extend(probs)\n","            del feats\n","            del labels\n","    return probs_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvfmL5fCcNym"},"source":["# Testing and generate probability"]},{"cell_type":"code","metadata":{"id":"PIIOa884iLly"},"source":["from sklearn import metrics\n","\n","def infer(model, loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    probs = []\n","    true_labels = []\n","\n","    with torch.no_grad():\n","        for inputs,labels in loader:\n","            inputs = inputs.float().to(device)\n","            labels = labels.long()\n","            outputs = model(inputs).cpu()\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            # print(outputs)\n","            # print(predicted)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            probs.extend(outputs.detach().numpy())\n","            true_labels.extend(labels.detach().numpy())\n","\n","    acc = correct / total\n","    avg_loss = running_loss / total\n","    return avg_loss, acc, probs, true_labels\n","\n","avg_loss, acc, probs, true_labels = infer(model_out, dataloaders_dict[\"val\"], criterion, device)\n","\n","probs = np.array(probs)\n","mod = np.array(probs[:,1])\n","true_labels = np.array(true_labels)\n","\n","# Compute ROC curve\n","fpr, tpr, thresholds = metrics.roc_curve(true_labels , mod )\n","\n","# Compute ROC area\n","roc_auc = auc(fpr, tpr)\n","print('ROC area is {0}'.format(roc_auc))\n","\n","# plt.figure()\n","# plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n","# plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n","# plt.xlim([-0.01, 1.0])\n","# plt.ylim([0.0, 1.01])\n","# plt.xlabel('False Positive Rate')\n","# plt.ylabel('True Positive Rate')\n","# plt.title('Receiver operating characteristic')\n","# plt.legend(loc=\"lower right\")\n","\n","# plt.savefig(dir + \"resnet_HVflip_AUC.jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QcT8aQJPZzE0"},"source":["# !unzip -qq /content/histopathologic-cancer-detection.zip \"sample_submission.csv\" -d /content/\n","\n","\n","# ID taken in order in test folder\n","ids = [name[0].split(\".tif\")[0].split(\"/\")[-1] for name in test_dataset.imgs]\n","\n","order_ids = []\n","with open(\"/content/sample_submission.csv\", \"r\") as w:\n","    w.readline()\n","    for line in w:\n","        id = line.split(\",\")[0]\n","        order_ids.append(id)\n","       \n","# model_load, input_size = initialize_model(\"resnet\", num_classes, feature_extract, use_pretrained=True)\n","# checkpoint = torch.load(\"/content/drive/MyDrive/Comp_Medicine/Model/Resnet18_HVflip2\")\n","# model_load.load_state_dict(checkpoint[\"model_state_dict\"])\n","# model_load.to(device)\n","\n","probs = test_classify(model_out, test_dataloader)\n","\n","with open(\"/content/submit_hvflip.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(order_ids)):\n","        id = order_ids[i]\n","        index = ids.index(id)\n","        f.write(ids[index] + \",\" + str(probs[index]) + \"\\n\" )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kfMUhNKArT8W"},"source":["# !less /content/drive/MyDrive/Comp_Medicine/submit_final.csv\n","!kaggle competitions submit -c histopathologic-cancer-detection -f \"/content/submit_hvflip.csv\" -m \"flip_immature\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qz-tmDqbtilB"},"source":[""],"execution_count":null,"outputs":[]}]}